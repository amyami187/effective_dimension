{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable, Tuple, List\n",
    "from copy import deepcopy\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from qiskit import QuantumRegister, ClassicalRegister, QuantumCircuit\n",
    "from qiskit import BasicAer, execute\n",
    "from qiskit.aqua.components.optimizers.cobyla import COBYLA\n",
    "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes, TwoLocal, ZFeatureMap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit.quantum_info import Statevector\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from qiskit import IBMQ, Aer\n",
    "\n",
    "from qiskit.aqua import aqua_globals, QuantumInstance\n",
    "from qiskit.aqua.components.optimizers import Optimizer\n",
    "from qiskit.aqua.components.optimizers import OptimizerSupportLevel\n",
    "\n",
    "from qiskit.aqua.components.optimizers.cobyla import COBYLA\n",
    "from qiskit.aqua.components.optimizers import Optimizer, OptimizerSupportLevel\n",
    "from qiskit.providers.aer.noise.noise_model import NoiseModel\n",
    "from qiskit.ignis.mitigation.measurement import CompleteMeasFitter\n",
    "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes, TwoLocal, ZFeatureMap\n",
    "from qiskit.aqua.operators import ListOp, CircuitSampler, VectorStateFn, StateFn, Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2020-10-14 08:25:17,055: Credentials are already in use. The existing account in the session will be replaced.\n"
     ]
    }
   ],
   "source": [
    "TOKEN = # insert token here\n",
    "IBMQ.save_account(TOKEN, overwrite=True)\n",
    "provider = IBMQ.load_account()\n",
    "#provider = IBMQ.get_provider(hub='ibm-q-internal', group='deployed', project='default')\n",
    "provider = IBMQ.get_provider(hub='ibm-q-internal', group='performance', project='paper-priority')\n",
    "\n",
    "backend_name = 'ibmq_montreal'\n",
    "backend_ibmq = provider.get_backend(backend_name)\n",
    "properties = backend_ibmq.properties()\n",
    "coupling_map = backend_ibmq.configuration().coupling_map\n",
    "noise_model = NoiseModel.from_backend(properties)\n",
    "layout = [0, 1, 2, 3]\n",
    "\n",
    "shots = 8000\n",
    "\n",
    "qi_ibmq = QuantumInstance(backend=backend_ibmq, optimization_level=3, shots=shots,\n",
    "                          skip_qobj_validation=False,\n",
    "                          seed_transpiler=2, measurement_error_mitigation_cls=CompleteMeasFitter,\n",
    "                          seed_simulator=2,\n",
    "                          measurement_error_mitigation_shots=8000, initial_layout=layout)\n",
    "\n",
    "qi = qi_ibmq\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<IBMQBackend('ibmq_athens') from IBMQ(hub='ibm-q-internal', group='performance', project='paper-priority')>,\n",
       " <IBMQBackend('ibmq_montreal') from IBMQ(hub='ibm-q-internal', group='performance', project='paper-priority')>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provider.backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADAM(Optimizer):\n",
    "    \"\"\"Adam and AMSGRAD optimizers.\n",
    "\n",
    "        Adam [1] is a gradient-based optimization algorithm that is relies on adaptive estimates of\n",
    "        lower-order moments. The algorithm requires little memory and is invariant to diagonal\n",
    "        rescaling of the gradients. Furthermore, it is able to cope with non-stationary objective\n",
    "        functions and noisy and/or sparse gradients.\n",
    "\n",
    "        AMSGRAD [2] (a variant of Adam) uses a 'long-term memory' of past gradients and, thereby,\n",
    "        improves convergence properties.\n",
    "\n",
    "        References:\n",
    "\n",
    "            [1]: Kingma, Diederik & Ba, Jimmy (2014), Adam: A Method for Stochastic Optimization.\n",
    "                 `arXiv:1412.6980 <https://arxiv.org/abs/1412.6980>`_\n",
    "\n",
    "            [2]: Sashank J. Reddi and Satyen Kale and Sanjiv Kumar (2018),\n",
    "                 On the Convergence of Adam and Beyond.\n",
    "                 `arXiv:1904.09237 <https://arxiv.org/abs/1904.09237>`_\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    _OPTIONS = ['maxiter', 'tol', 'lr', 'beta_1', 'beta_2',\n",
    "                'noise_factor', 'eps', 'amsgrad', 'snapshot_dir']\n",
    "\n",
    "    def __init__(self,\n",
    "                 maxiter: int = 10000,\n",
    "                 tol: float = 1e-6,\n",
    "                 lr: float = 1e-3,\n",
    "                 beta_1: float = 0.9,\n",
    "                 beta_2: float = 0.99,\n",
    "                 noise_factor: float = 1e-8,\n",
    "                 eps: float = 1e-10,\n",
    "                 amsgrad: bool = False,\n",
    "                 snapshot_dir: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            maxiter: Maximum number of iterations\n",
    "            tol: Tolerance for termination\n",
    "            lr: Value >= 0, Learning rate.\n",
    "            beta_1: Value in range 0 to 1, Generally close to 1.\n",
    "            beta_2: Value in range 0 to 1, Generally close to 1.\n",
    "            noise_factor: Value >= 0, Noise factor\n",
    "            eps : Value >=0, Epsilon to be used for finite differences if no analytic\n",
    "                gradient method is given.\n",
    "            amsgrad: True to use AMSGRAD, False if not\n",
    "            snapshot_dir: If not None save the optimizer's parameter\n",
    "                after every step to the given directory\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        for k, v in locals().items():\n",
    "            if k in self._OPTIONS:\n",
    "                self._options[k] = v\n",
    "        self._maxiter = maxiter\n",
    "        self._snapshot_dir = snapshot_dir\n",
    "        self._tol = tol\n",
    "        self._lr = lr\n",
    "        self._beta_1 = beta_1\n",
    "        self._beta_2 = beta_2\n",
    "        self._noise_factor = noise_factor\n",
    "        self._eps = eps\n",
    "        self._amsgrad = amsgrad\n",
    "        self.loss_list = []\n",
    "\n",
    "        # runtime variables\n",
    "        self._t = 0  # time steps\n",
    "        self._m = np.zeros(1)\n",
    "        self._v = np.zeros(1)\n",
    "        if self._amsgrad:\n",
    "            self._v_eff = np.zeros(1)\n",
    "\n",
    "        if self._snapshot_dir:\n",
    "\n",
    "            with open(os.path.join(self._snapshot_dir, 'adam_params.csv'), mode='w') as csv_file:\n",
    "                if self._amsgrad:\n",
    "                    fieldnames = ['v', 'v_eff', 'm', 't']\n",
    "                else:\n",
    "                    fieldnames = ['v', 'm', 't']\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "\n",
    "    def get_support_level(self):\n",
    "        \"\"\" Return support level dictionary \"\"\"\n",
    "        return {\n",
    "            'gradient': OptimizerSupportLevel.supported,\n",
    "            'bounds': OptimizerSupportLevel.ignored,\n",
    "            'initial_point': OptimizerSupportLevel.supported\n",
    "        }\n",
    "\n",
    "    def save_params(self, snapshot_dir: str) -> None:\n",
    "        \"\"\"Save the current iteration parameters to a file called ``adam_params.csv``.\n",
    "\n",
    "        Note:\n",
    "\n",
    "            The current parameters are appended to the file, if it exists already.\n",
    "            The file is not overwritten.\n",
    "\n",
    "        Args:\n",
    "            snapshot_dir: The directory to store the file in.\n",
    "        \"\"\"\n",
    "        if self._amsgrad:\n",
    "            with open(os.path.join(snapshot_dir, 'adam_params.csv'), mode='a') as csv_file:\n",
    "                fieldnames = ['v', 'v_eff', 'm', 't']\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                writer.writerow({'v': self._v, 'v_eff': self._v_eff,\n",
    "                                 'm': self._m, 't': self._t})\n",
    "        else:\n",
    "            with open(os.path.join(snapshot_dir, 'adam_params.csv'), mode='a') as csv_file:\n",
    "                fieldnames = ['v', 'm', 't']\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                writer.writerow({'v': self._v, 'm': self._m, 't': self._t})\n",
    "\n",
    "    def load_params(self, load_dir: str) -> None:\n",
    "        \"\"\"Load iteration parameters for a file called ``adam_params.csv``.\n",
    "\n",
    "        Args:\n",
    "            load_dir: The directory containing ``adam_params.csv``.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(load_dir, 'adam_params.csv'), mode='r') as csv_file:\n",
    "            if self._amsgrad:\n",
    "                fieldnames = ['v', 'v_eff', 'm', 't']\n",
    "            else:\n",
    "                fieldnames = ['v', 'm', 't']\n",
    "            reader = csv.DictReader(csv_file, fieldnames=fieldnames)\n",
    "            for line in reader:\n",
    "                v = line['v']\n",
    "                if self._amsgrad:\n",
    "                    v_eff = line['v_eff']\n",
    "                m = line['m']\n",
    "                t = line['t']\n",
    "\n",
    "        v = v[1:-1]\n",
    "        self._v = np.fromstring(v, dtype=float, sep=' ')\n",
    "        if self._amsgrad:\n",
    "            v_eff = v_eff[1:-1]\n",
    "            self._v_eff = np.fromstring(v_eff, dtype=float, sep=' ')\n",
    "        m = m[1:-1]\n",
    "        self._m = np.fromstring(m, dtype=float, sep=' ')\n",
    "        t = t[1:-1]\n",
    "        self._t = np.fromstring(t, dtype=int, sep=' ')\n",
    "\n",
    "    def minimize(self, objective_function: Callable[[np.ndarray], float], initial_point: np.ndarray,\n",
    "                 gradient_function: Callable[[np.ndarray], float]) -> Tuple[np.ndarray, float, int]:\n",
    "        \"\"\"Run the minimization.\n",
    "\n",
    "        Args:\n",
    "            objective_function: A function handle to the objective function.\n",
    "            initial_point: The initial iteration point.\n",
    "            gradient_function: A function handle to the gradient of the objective function.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of (optimal parameters, optimal value, number of iterations).\n",
    "        \"\"\"\n",
    "        derivative = gradient_function(initial_point)\n",
    "        self._t = 0\n",
    "        self._m = np.zeros(np.shape(derivative))\n",
    "        self._v = np.zeros(np.shape(derivative))\n",
    "        if self._amsgrad:\n",
    "            self._v_eff = np.zeros(np.shape(derivative))\n",
    "        self.loss_list = []\n",
    "        params = params_new = initial_point\n",
    "        while self._t < self._maxiter:\n",
    "            derivative = gradient_function(params)\n",
    "            self._t += 1\n",
    "            self._m = self._beta_1 * self._m + (1 - self._beta_1) * derivative\n",
    "            self._v = self._beta_2 * self._v + (1 - self._beta_2) * derivative * derivative\n",
    "            lr_eff = self._lr * np.sqrt(1 - self._beta_2 ** self._t) / (1 - self._beta_1 ** self._t)\n",
    "            if not self._amsgrad:\n",
    "                params_new = (params - lr_eff * self._m.flatten()\n",
    "                              / (np.sqrt(self._v.flatten()) + self._noise_factor))\n",
    "                self.loss_list.append(objective_function(params_new))\n",
    "            else:\n",
    "                self._v_eff = np.maximum(self._v_eff, self._v)\n",
    "                params_new = (params - lr_eff * self._m.flatten()\n",
    "                              / (np.sqrt(self._v_eff.flatten()) + self._noise_factor))\n",
    "                self.loss_list.append(objective_function(params_new))\n",
    "            if self._snapshot_dir:\n",
    "                self.save_params(self._snapshot_dir)\n",
    "            if np.linalg.norm(params - params_new) < self._tol:\n",
    "                return params_new, objective_function(params_new), self._t\n",
    "            else:\n",
    "                params = params_new\n",
    "        return params_new, objective_function(params_new), self._t\n",
    "\n",
    "    def optimize(self, num_vars: int, objective_function: Callable[[np.ndarray], float],\n",
    "                 gradient_function: Optional[Callable[[np.ndarray], float]] = None,\n",
    "                 variable_bounds: Optional[List[Tuple[float, float]]] = None,\n",
    "                 initial_point: Optional[np.ndarray] = None\n",
    "                 ) -> Tuple[np.ndarray, float, int]:\n",
    "        \"\"\"Perform optimization.\n",
    "\n",
    "        Args:\n",
    "            num_vars: Number of parameters to be optimized.\n",
    "            objective_function: Handle to a function that computes the objective function.\n",
    "            gradient_function: Handle to a function that computes the gradient of the objective\n",
    "                function.\n",
    "            variable_bounds: deprecated\n",
    "            initial_point: The initial point for the optimization.\n",
    "\n",
    "        Returns:\n",
    "            A tuple (point, value, nfev) where\\n\n",
    "                point: is a 1D numpy.ndarray[float] containing the solution\\n\n",
    "                value: is a float with the objective function value\\n\n",
    "                nfev: is the number of objective function calls\n",
    "        \"\"\"\n",
    "        super().optimize(num_vars, objective_function, gradient_function,\n",
    "                         variable_bounds, initial_point)\n",
    "        if initial_point is None:\n",
    "            initial_point = aqua_globals.random.random(num_vars)\n",
    "        if gradient_function is None:\n",
    "            gradient_function = Optimizer.wrap_function(Optimizer.gradient_num_diff,\n",
    "                                                        (objective_function, self._eps))\n",
    "\n",
    "        point, value, nfev = self.minimize(objective_function, initial_point, gradient_function)\n",
    "        return point, value, nfev, self.loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_sim = False\n",
    "\n",
    "# size of training data set\n",
    "training_size = 100\n",
    "\n",
    "# dimension of data sets\n",
    "n = 4\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# load iris and normalise\n",
    "x = preprocessing.normalize(iris.data)\n",
    "\n",
    "x1_train = x[0:49, :] # class A\n",
    "x2_train = x[50:99, :] # class B\n",
    "\n",
    "training_input = {'A':x1_train, 'B':x2_train}\n",
    "class_labels = ['A', 'B']\n",
    "\n",
    "blocks = 1\n",
    "sv = Statevector.from_label('0' * n)\n",
    "# circuit = QuantumCircuit(n)\n",
    "feature_map = ZZFeatureMap(n, reps=2, entanglement='linear')\n",
    "var_form = RealAmplitudes(n, reps=blocks, entanglement='linear')\n",
    "circuit = feature_map.combine(var_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(params, x):\n",
    "    \"\"\"Get the parameters dict for the circuit\"\"\"\n",
    "    parameters = {}\n",
    "    for i, p in enumerate(feature_map.ordered_parameters):\n",
    "        parameters[p] = x[i]\n",
    "    for i, p in enumerate(var_form.ordered_parameters):\n",
    "        parameters[p] = params[i]\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def assign_label(bit_string, class_labels):\n",
    "    hamming_weight = sum([int(k) for k in list(bit_string)])\n",
    "    is_odd_parity = hamming_weight & 1\n",
    "    if is_odd_parity:\n",
    "        return class_labels[1]\n",
    "    else:\n",
    "        return class_labels[0]\n",
    "\n",
    "def return_probabilities(counts, class_labels):\n",
    "    result = {class_labels[0]: 0,\n",
    "              class_labels[1]: 0}\n",
    "    for key, item in counts.items():\n",
    "        label = assign_label(key, class_labels)\n",
    "        result[label] += counts[key]\n",
    "    return result\n",
    "\n",
    "\n",
    "def classify(x_list, params, class_labels):\n",
    "    qc = deepcopy(circuit)\n",
    "    if not sv_sim:\n",
    "        qc.measure_all()\n",
    "    qc_list = []\n",
    "    for i, x in enumerate(x_list):\n",
    "        circ_ = qc.assign_parameters(get_data_dict(params, x))\n",
    "        circ_.name = 'circ' + str(i)\n",
    "        if sv_sim:\n",
    "            circ_ = sv.evolve(circ_)\n",
    "        qc_list += [circ_]\n",
    "    if not sv_sim:\n",
    "        print('sent circuits (%s)' % len(qc_list))\n",
    "        results = qi.execute(qc_list)\n",
    "    probs = []\n",
    "    # TODO execute qc_list\n",
    "    for i in range(len(qc_list)):\n",
    "        if sv_sim:\n",
    "            counts = qc.to_counts()\n",
    "        else:\n",
    "            counts = results.get_counts(qc_list[i])\n",
    "            # print('counts ', counts)\n",
    "        counts = {k: v / sum(counts.values()) for k, v in counts.items()}\n",
    "        prob = return_probabilities(counts, class_labels)\n",
    "        probs += [prob]\n",
    "    return probs\n",
    "\n",
    "def grad_classify(x_list, params, class_labels):\n",
    "    qc = deepcopy(circuit)\n",
    "    qc_list = []\n",
    "    for x in x_list:\n",
    "        parameters = {}\n",
    "        for i, p in enumerate(feature_map.ordered_parameters):\n",
    "            parameters[p] = x[i]\n",
    "        circ_ = qc.assign_parameters(parameters)\n",
    "        if sv_sim:\n",
    "            raise TypeError('For now the gradient implementation only allows for Aer backends.')\n",
    "        qc_list += [StateFn(circ_)]\n",
    "    if not sv_sim:\n",
    "        qc_list = ListOp(qc_list)\n",
    "        grad_fn = Gradient(method='lin_comb').gradient_wrapper(qc_list, var_form.ordered_parameters,\n",
    "                                                               backend=qi)\n",
    "        print('send gradient circuits')\n",
    "        grad = grad_fn(params)\n",
    "    probs = []\n",
    "    for grad_vec in grad:\n",
    "        prob = []\n",
    "        for i, qc in enumerate(qc_list):\n",
    "            counts = VectorStateFn(grad_vec[i]).to_dict_fn().primitive\n",
    "            prob += [return_probabilities(counts, class_labels)]\n",
    "        probs += [prob]\n",
    "    return probs\n",
    "\n",
    "from math import log\n",
    "\n",
    "\n",
    "def CrossEntropy(yHat, y):\n",
    "    if y == 'A':\n",
    "        return -log(yHat['A'])\n",
    "    else:\n",
    "        return -log(1-yHat['A'])\n",
    "\n",
    "\n",
    "def grad_CrossEntropy(yHat, y, yHat_grad):\n",
    "    if y == 'A':\n",
    "        return -yHat_grad['A']/(yHat['A'])\n",
    "    else:\n",
    "        return yHat_grad['A']/((1-yHat['A']))\n",
    "\n",
    "\n",
    "def cost_function(training_input, class_labels, params, shots=100, print_value=False):\n",
    "    # map training input to list of labels and list of samples\n",
    "    cost = 0\n",
    "    training_labels = []\n",
    "    training_samples = []\n",
    "    for label, samples in training_input.items():\n",
    "        for sample in samples:\n",
    "            training_labels += [label]\n",
    "            training_samples += [sample]\n",
    "\n",
    "    # classify all samples\n",
    "    probs = classify(training_samples, params, class_labels)\n",
    "\n",
    "    # evaluate costs for all classified samples\n",
    "    for i, prob in enumerate(probs):\n",
    "        # cost += cost_estimate_sigmoid(prob, training_labels[i])\n",
    "        cost += CrossEntropy(yHat=prob, y=training_labels[i])\n",
    "    cost /= len(training_samples)\n",
    "\n",
    "    # print resulting objective function\n",
    "    # if print_value:\n",
    "    #    print('%.4f' % cost)\n",
    "\n",
    "    # return objective value\n",
    "    print('Cost %.4f' % cost)\n",
    "    return cost\n",
    "\n",
    "def grad_cost_function(training_input, class_labels, params, shots=100, print_value=False):\n",
    "    # map training input to list of labels and list of samples\n",
    "    grad_cost = np.zeros(len(params))\n",
    "    training_labels = []\n",
    "    training_samples = []\n",
    "    for label, samples in training_input.items():\n",
    "        for sample in samples:\n",
    "            training_labels += [label]\n",
    "            training_samples += [sample]\n",
    "\n",
    "    # classify all samples\n",
    "    probs = classify(training_samples, params, class_labels)\n",
    "    grad_probs = grad_classify(training_samples, params, class_labels)\n",
    "    # grad_probs = list(map(list, zip(*grad_probs))) #transpose\n",
    "    # evaluate costs for all classified samples\n",
    "    for j, grad_prob in enumerate(grad_probs):\n",
    "        for i, prob in enumerate(probs):\n",
    "            grad_cost[j] += grad_CrossEntropy(yHat=prob, y=training_labels[i],\n",
    "                                              yHat_grad=grad_prob[i]) / len(training_samples)\n",
    "\n",
    "    # print resulting objective function\n",
    "    # if print_value:\n",
    "    #    print('%.4f' % cost)\n",
    "\n",
    "    # return objective value\n",
    "    print('Gradient', grad_cost)\n",
    "    return grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the optimizer\n",
    "optimizer = ADAM(maxiter=100, lr=0.1)\n",
    "\n",
    "# define objective function for training\n",
    "objective_function = lambda params: cost_function(training_input, class_labels, params, print_value=True)\n",
    "\n",
    "# define function for training\n",
    "grad_function = lambda params: grad_cost_function(training_input, class_labels, params, print_value=True)\n",
    "\n",
    "for i in range(100):\n",
    "    np.random.seed(i)\n",
    "    d = 8  # num of trainable params\n",
    "\n",
    "    # train classifier\n",
    "    init_params = 2 * np.pi * np.random.rand(n * (1) * 2)\n",
    "    opt_params, value, _, loss = optimizer.optimize(len(init_params), objective_function,\n",
    "                                                    gradient_function=grad_function,\n",
    "                                                    initial_point=init_params)\n",
    "\n",
    "    # print results\n",
    "\n",
    "    f1 = 'quantum_loss_hard_dep2_%d.npy' %i\n",
    "    np.save(f1, loss)\n",
    "    f2 = 'opt_params_hard_dep2_%d.npy'%i\n",
    "    np.save(f2, opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
